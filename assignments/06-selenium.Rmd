---
title: "Web Scraping: Assignment 6"
author: "Simon Munzert"
output: html_document
---


### 0. Preparation: Load packages

```{r load packages}
# Enter R code here
```


### 1. Accessing data from a dynamic webpage

*Note: If you are not able to get this code compiled using `knitr`, or if you fail at setting up Selenium on your machine, simply document your code and set `eval = FALSE` in the code snippet header*.

In the following, use `RSelenium` together with Selenium to run a search query on Google Trends. To that end, implement the following steps:

a. Launch a Selenium driver session and navigate to "https://trends.google.com/trends/".
b. Run a search for "data science".
c. Once you are on the Results page, add another keyword for comparison, "rocket science". You might need the `sendKeysToActiveElement()` function together with the `key = "enter"` functionality to get this running. Important note: this step causes trouble when knitting the document. Just write down the needed lines and then comment them out before knitting.
d. Download the CSV file that contains the data on the interest in these terms over time.
e. Store the live DOM tree in an HTML file on your local drive.
f. Close the connection.
g. Parse the downloaded CSV into a well-formatted data.frame and visualize the time series for "data science" in a plot.

```{r, eval = TRUE}
# Enter R code here
```


### 2. Writing your own robots.txt file

Write your own `robots.txt` file providing the following rules:

a. The Googlebot is not allowed to crawl your website.
b. Scraping your `/private/` folder is generally not allowed.
c. The Openbot is allowed to crawl the `/private/images folder at a crawl-delay rate of 1 second.
d. You leave a comment in the txt that asks people interested in crawling the page to get in touch with you via your (fictional) email address.

Use the following text box to document your file:


```{}
# Enter robots.txt code here
```



### 3. Working with the robotstxt package

Inform yourself about the robotstxt package and install it. Using this package, solve the following tasks:

a. Load the package. Then, use package functions to retrieve the `robots.txt` from the washingtonpost.com website and to parse the file.
b. Provide a list of User-agents that are addressed in the `robots.txt`.
c. Using the data that is provided in the parsed `robots.txt`, check which bot has the most "`Disallows"!
d. Check whether a generic bot is allowed to crawl data from the following directories: `"/todays_paper/"`, `"/jobs/"`, and "/politics/"`.

```{r}
# Enter R code here
```



