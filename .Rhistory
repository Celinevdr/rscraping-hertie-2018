summary(tor_item)
name(tor_item)
names(tor_item)
tor_item[[1]]
?get_item
# extract claims ("features")
extract_claims(tor_item, "P17")
# extract properties
property <- get_property("Q183")
str(property)
# extract properties
browseURL("https://www.wikidata.org/wiki/Q183")
property <- get_property("Q82425")
str(property)
# free, collaborative, multilingual database
# supports Wikis of the Wikimedia movement (such as Wikipedia or Wikimedia Commons) by offering standardized storage and access to data
# basic logic:
# basic objects are stored as items with a unique item number. Example: Douglas Adams --> Q42
browseURL("https://www.wikidata.org/wiki/Q42")
# search Wikipedia with search term
searchWikiFun <- function(term = NULL, limit = 100, title.only = TRUE, wordcount.min = 500) {
# API doc at https://www.mediawiki.org/wiki/API:Search
term <- URLencode(term)
url <- sprintf("https://de.wikipedia.org/w/api.php?action=query&list=search&srsearch=%s&srlimit=%d&format=json", term, limit)
wiki_search_parsed <- jsonlite::fromJSON(url)$query$search
wiki_search_parsed <- dplyr::filter(wiki_search_parsed, wordcount >= wordcount.min)
if(title.only == FALSE) {
return(wiki_search_parsed)
} else{
return(wiki_search_parsed$title)
}
}
# test
pages_search <- searchWikiFun(term = "Arbeitslosigkeit", limit = 100, wordcount.min = 500)
pages_search
?article_pageviews
# quick assessment of pages' recent pageview statistics
pagesMinPageviews <- function(pages = NULL, start = "2016090100", end = "2016093000", min.dailyviewsavg = 50) {
pageviews_list <- list()
pageviews_mean <- numeric()
for (i in seq_along(pages)) {
pageviews_list[[i]] <- try(article_pageviews(project = "de.wikipedia", article = URLencode(pages[i]), start = start, end = end, reformat = TRUE))
pageviews_mean[i] <- try(mean(pageviews_list[[i]]$views, na.rm = TRUE))
}
pageviews_mean_df <- data.frame(page = pages, pageviews_mean = num(pageviews_mean), stringsAsFactors = FALSE)
pages_minviews <- dplyr::filter(pageviews_mean_df, pageviews_mean >= min.dailyviewsavg) %>% extract2("page")
return(pages_minviews)
}
pagenames_arbeitslosigkeit <- pagesMinPageviews(pages = pages_search, start = "2016090100", end = "2016093000", min.dailyviewsavg = 50)
?article_pageviews
# quick assessment of pages' recent pageview statistics
pagesMinPageviews <- function(pages = NULL, start = "2018010100", end = "2018013100", min.dailyviewsavg = 50) {
pageviews_list <- list()
pageviews_mean <- numeric()
for (i in seq_along(pages)) {
pageviews_list[[i]] <- try(article_pageviews(project = "de.wikipedia", article = URLencode(pages[i]), start = start, end = end, reformat = TRUE))
pageviews_mean[i] <- try(mean(pageviews_list[[i]]$views, na.rm = TRUE))
}
pageviews_mean_df <- data.frame(page = pages, pageviews_mean = num(pageviews_mean), stringsAsFactors = FALSE)
pages_minviews <- dplyr::filter(pageviews_mean_df, pageviews_mean >= min.dailyviewsavg) %>% extract2("page")
return(pages_minviews)
}
pagenames_arbeitslosigkeit <- pagesMinPageviews(pages = pages_search, start = "2016090100", end = "2016093000", min.dailyviewsavg = 50)
# test
pages_search <- searchWikiFun(term = "Arbeitslosigkeit", limit = 100, wordcount.min = 500)
pages = pages_search[1]
pages
start = "2018010100"
end = "2018013100"
min.dailyviewsavg = 50
pageviews_list <- list()
pageviews_mean <- numeric()
i=1
pageviews_list[[i]] <- try(article_pageviews(project = "de.wikipedia", article = URLencode(pages[i]), start = start, end = end, reformat = TRUE))
pageviews_mean[i] <- try(mean(pageviews_list[[i]]$views, na.rm = TRUE))
pageviews_list
pageviews_mean
pageviews_mean_df <- data.frame(page = pages, pageviews_mean = num(pageviews_mean), stringsAsFactors = FALSE)
pageviews_mean_df <- data.frame(page = pages, pageviews_mean = as.numeric(pageviews_mean), stringsAsFactors = FALSE)
pageviews_mean_df
pages_minviews <- dplyr::filter(pageviews_mean_df, pageviews_mean >= min.dailyviewsavg) %>% extract2("page")
# quick assessment of pages' recent pageview statistics
pagesMinPageviews <- function(pages = NULL, start = "2018010100", end = "2018013100", min.dailyviewsavg = 50) {
pageviews_list <- list()
pageviews_mean <- numeric()
for (i in seq_along(pages)) {
pageviews_list[[i]] <- try(article_pageviews(project = "de.wikipedia", article = URLencode(pages[i]), start = start, end = end, reformat = TRUE))
pageviews_mean[i] <- try(mean(pageviews_list[[i]]$views, na.rm = TRUE))
}
pageviews_mean_df <- data.frame(page = pages, pageviews_mean = as.numeric(pageviews_mean), stringsAsFactors = FALSE)
pages_minviews <- dplyr::filter(pageviews_mean_df, pageviews_mean >= min.dailyviewsavg) %>% extract2("page")
return(pages_minviews)
}
pagenames_arbeitslosigkeit <- pagesMinPageviews(pages = pages_search, start = "2016090100", end = "2016093000", min.dailyviewsavg = 50)
pagenames_arbeitslosigkeit
View(pagenames_arbeitslosigkeit)
pages_search
pagenames_arbeitslosigkeit <- pagesMinPageviews(pages = pages_search, start = "2018010100", end = "2018013100", min.dailyviewsavg = 50)
pagenames_arbeitslosigkeit
dir()
dir(data)
list.files/data
list.files(data)
list.files(/data)
list.files("data")
list.files("data/wikipageviews")
# download pageview statistics with wikipediatrend package
pageviewsDownload <- function(pages = NULL, folder = "~", from = "2015070100", to = "2017040100", language = "de") {
pageviews_list <- list()
pageviews_filenames_raw <- vector()
for (i in seq_along(pages)) {
filename <- paste0("wp_", pages[i], "_", language, ".csv")
if (!file.exists(paste0(folder, filename))) {
pageviews_list[[i]] <- try(article_pageviews(project = "de.wikipedia", article =  URLencode(pages[i]), start = from, end = to))
try(write.csv(pageviews_list[[i]], file = paste0(folder, filename), row.names = FALSE))
}
pageviews_filenames_raw[i] <- filename
}
}
# download pageview statistics with wikipediatrend package
pageviewsDownload <- function(pages = NULL, folder = "~", from = "2017070100", to = "2018040100", language = "de") {
pageviews_list <- list()
pageviews_filenames_raw <- vector()
for (i in seq_along(pages)) {
filename <- paste0("wp_", pages[i], "_", language, ".csv")
if (!file.exists(paste0(folder, filename))) {
pageviews_list[[i]] <- try(article_pageviews(project = "de.wikipedia", article =  URLencode(pages[i]), start = from, end = to))
try(write.csv(pageviews_list[[i]], file = paste0(folder, filename), row.names = FALSE))
}
pageviews_filenames_raw[i] <- filename
}
}
pageviewsDownload(pages = pagenames_arbeitslosigkeit, folder = "data/wikipageviews/", from = "2015070100", to = "2017040100", language = "de")
trump_views <- article_pageviews(project = "en.wikipedia", article = "Donald Trump", user_type = "user", start = "2015070100", end = "2017040100")
head(trump_views)
clinton_views <- article_pageviews(project = "en.wikipedia", article = "Hillary Clinton", user_type = "user", start = "2015070100", end = "2017040100")
plot(ymd(trump_views$date), trump_views$views, col = "red", type = "l")
lines(ymd(clinton_views$date), clinton_views$views, col = "blue")
german_parties_views <- article_pageviews(
project = "de.wikipedia",
article = c("Christlich Demokratische Union Deutschlands", "Christlich-Soziale Union in Bayern", "Sozialdemokratische Partei Deutschlands", "Freie Demokratische Partei", "Bündnis 90/Die Grünen", "Die Linke", "Alternative für Deutschland"),
user_type = "user",
start = "2015090100",
end = "2017040100"
)
table(german_parties_views$article)
parties <- unique(german_parties_views$article)
dat <- filter(german_parties_views, article == parties[1])
plot(ymd(dat$date), dat$views, col = "black", type = "l")
dat <- filter(german_parties_views, article == parties[2])
lines(ymd(dat$date), dat$views, col = "blue")
dat <- filter(german_parties_views, article == parties[3])
lines(ymd(dat$date), dat$views, col = "red")
dat <- filter(german_parties_views, article == parties[7])
lines(ymd(dat$date), dat$views, col = "brown")
parties
# test
pages_search <- searchWikiFun(term = "Arbeitslosigkeit", limit = 100, wordcount.min = 500)
pageviewsDownload(pages = pages_search, folder = "data/wikipageviews/", from = "2017070100", to = "2017040100", language = "de")
pages_search
# download pageview statistics with wikipediatrend package
pageviewsDownload <- function(pages = NULL, folder = "~", from = "2015070100", to = "2018040100", language = "de") {
pageviews_list <- list()
pageviews_filenames_raw <- vector()
for (i in seq_along(pages)) {
filename <- paste0("wp_", pages[i], "_", language, ".csv")
if (!file.exists(paste0(folder, filename))) {
pageviews_list[[i]] <- try(article_pageviews(project = "de.wikipedia", article =  URLencode(pages[i]), start = from, end = to))
try(write.csv(pageviews_list[[i]], file = paste0(folder, filename), row.names = FALSE))
}
pageviews_filenames_raw[i] <- filename
}
}
# test
pages_search <- searchWikiFun(term = "Arbeitslosigkeit", limit = 100, wordcount.min = 500)
# test
pages_search <- searchWikiFun(term = "Arbeitslosigkeit", limit = 100, wordcount.min = 500)
dir.create("data/wikipageviews")
pageviewsDownload(pages = pages_search, folder = "data/wikipageviews/", from = "2017070100", to = "2018040100", language = "de")
# download pageview statistics with wikipediatrend package
pageviewsDownload <- function(pages = NULL, folder = "~", from = "2015070100", to = "2018040100", language = "de") {
pageviews_list <- list()
pageviews_filenames_raw <- vector()
for (i in seq_along(pages)) {
filename <- paste0("wp_", pages[i], "_", language, ".csv")
if (!file.exists(paste0(folder, filename))) {
pageviews_list[[i]] <- try(article_pageviews(project = "de.wikipedia", article =  URLencode(pages[i]), start = from, end = to))
try(write.csv(pageviews_list[[i]], file = paste0(folder, filename), row.names = FALSE))
}
pageviews_filenames_raw[i] <- filename
}
}
pageviews_filenames_raw
# how to register at Twitter as developer, obtain and use access tokens
browseURL("https://mkearney.github.io/rtweet/articles/auth.html")
source("packages.r")
## api key (example below is not a real key)
load("/Users/munzerts/rkeys.RDa") # <--- adapt path here; see above!
## api key (example below is not a real key)
load("/Users/s.munzert/rkeys.RDa") # <--- adapt path here; see above!
TwitterToR_twitterkey
appname
## name assigned to created app
appname <- "TwitterToR" # <--- add your Twitter App name here!
## register app
twitter_token <- create_token(
app = appname,
consumer_key = TwitterToR_twitterkey,
consumer_secret = TwitterToR_twittersecret)
## check if everything worked
rt <- search_tweets("merkel", n = 200, token = twitter_token)
View(rt)
source("packages.r")
library(rtweet)
rt <- search_tweets("merkel", n = 1000, include_rts = TRUE, lang = "de", token = twitter_token)
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
## name assigned to created app
appname <- "TwitterToR"
## api key (example below is not a real key)
load("/Users/munzerts/rkeys.RDa")
## api key (example below is not a real key)
load("/Users/s.munzert/rkeys.RDa")
key <- TwitterToR_twitterkey
## api secret (example below is not a real key)
secret <- TwitterToR_twittersecret
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
rt <- search_tweets("merkel", n = 1000, include_rts = TRUE, lang = "de", token = twitter_token)
## name assigned to created app
appname <- "TwitterToR"
key <- TwitterToR_twitterkey
## api secret (example below is not a real key)
secret <- TwitterToR_twittersecret
twitter_token <- create_token(
app = appname,
consumer_key = key,
consumer_secret = secret)
## name assigned to created app
appname <- "TwitterToR" # <--- add your Twitter App name here!
## api key (example below is not a real key)
load("/Users/s.munzert/rkeys.RDa") # <--- adapt path here; see above!
## register app
twitter_token <- create_token(
app = appname,
consumer_key = TwitterToR_twitterkey,
consumer_secret = TwitterToR_twittersecret)
source("packages.r")
library(rtweet)
# again: how to register at Twitter as developer, obtain and use access tokens
browseURL("https://mkearney.github.io/rtweet/articles/auth.html")
## name assigned to created app
appname <- "TwitterToR" # <--- add your Twitter App name here!
## api key (example below is not a real key)
load("/Users/s.munzert/rkeys.RDa") # <--- adapt path here; see above!
## register app
twitter_token <- create_token(
app = appname,
consumer_key = TwitterToR_twitterkey,
consumer_secret = TwitterToR_twittersecret)
TwitterToR_twitterkey
TwitterToR_twittersecret
## register app
twitter_token <- create_token(
app = appname,
consumer_key = TwitterToR_twitterkey,
consumer_secret = TwitterToR_twittersecret,
set_renv = FALSE)
?create_token
rt <- search_tweets("merkel", n = 1000, include_rts = TRUE, lang = "de", token = twitter_token)
tauber_bad <- search_tweets(URLencode("tauber :("), n = 100, include_rts = FALSE, lang = "de", token = twitter_token)
tauber_good <- search_tweets(URLencode("tauber :)"), n = 100, include_rts = FALSE, lang = "de", token = twitter_token)
tauber_bad
rt <- search_tweets("merkel", n = 1000, include_rts = TRUE, lang = "de", token = twitter_token)
tauber_bad <- search_tweets(URLencode("storch :("), n = 100, include_rts = FALSE, lang = "de", token = twitter_token)
merkel <- search_tweets("merkel", n = 1000, include_rts = FALSE, lang = "de", token = twitter_token)
storch_bad <- search_tweets(URLencode("storch :("), n = 100, include_rts = FALSE, lang = "de", token = twitter_token)
storch_bad <- search_tweets(URLencode("storch :)"), n = 100, include_rts = FALSE, lang = "de", token = twitter_token)
storch_good <- search_tweets(URLencode("storch :)"), n = 100, include_rts = FALSE, lang = "de", token = twitter_token)
storch_bad <- search_tweets(URLencode("storch :("), n = 100, include_rts = FALSE, lang = "de", token = twitter_token)
storch_bad
View(storch_bad)
storch_bad$text
library(stringi)
stri_unescape_unicode("\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923")
stri_escape_unicode("\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923\U0001f923")
storch_good
View(storch_good)
# two APIs types of interest:
# REST APIs --> reading/writing/following/etc., "Twitter remote control"
browseURL("https://dev.twitter.com/rest/public/search")
dim(merkel)
names(merkel)
View(merkel)
# set keywords used to filter tweets
q <- paste0("clinton,trump,hillaryclinton,imwithher,realdonaldtrump,maga,electionday")
q <- paste0("schulz,merkel,btw17,btw2017")
# parse directly into data frame
twitter_stream_ger <- stream_tweets(q = q, timeout = 30, token = twitter_token)
q <- paste0("merkel,trump,macron")
# parse directly into data frame
twitter_stream <- stream_tweets(q = q, timeout = 30, token = twitter_token)
class8twitter_stream
class(twitter_stream)
View(twitter_stream)
# set up directory and JSON dump
rtweet.folder <- "data/rtweet-data"
dir.create(rtweet.folder)
streamname <- "politicians"
filename <- file.path(rtweet.folder, paste0(streamname, "_", format(Sys.time(), "%F-%H-%M-%S"), ".json"))
# create file with stream's meta data
streamtime <- format(Sys.time(), "%F-%H-%M-%S")
metadata <- paste0(
"q = ", q, "\n",
"streamtime = ", streamtime, "\n",
"filename = ", filename)
metafile <- gsub(".json$", ".txt", filename)
cat(metadata, file = metafile)
# sink stream into JSON file
stream_tweets(q = q, parse = FALSE,
timeout = 30,
file_name = filename,
language = "de",
token = twitter_token)
# parse from json file
rt <- parse_stream(filename)
# parse from json file
rt <- parse_stream(filename)
# inspect tweets data
names(rt)
head(rt)
q
# inspect users data
users_data(rt) %>% head()
users_data(rt) %>% names()
str(rt)
# sooo much. for some inspiration, check out
browseURL("http://pablobarbera.com/big-data-upf/")
?get_timeline
follow <- get_followers("AnaKubli")
follow
friends <- get_friends("AnaKubli")
friends
user_df <- lookup_users("AnaKubli")
names(user_df)
user_df <- lookup_users("AnaKubli")$user_id
user_df
twitter_names <- c("AnaKubli", "annapellegatta", "caromatamoros", "cusimanof",
"dgohla", "jonvrushi", "luciacizmaziova", "nadinaiacob",
"PresRamirez", "RubenZoest", "simonsaysnothin", "sjash87",
"donata64", "RummelJa", "bernstmeng", "CStinshoff")
followers <- get_followers("AnaKubli")
followers
twitter_names <- c("AnaKubli", "annapellegatta", "caromatamoros", "cusimanof",
"dgohla", "jonvrushi", "luciacizmaziova", "nadinaiacob",
"PresRamirez", "RubenZoest", "simonsaysnothin", "sjash87",
"donata64", "RummelJa", "bernstmeng", "CStinshoff")
user_id_list <- list()
user_followers <- list()
user_friends <- list()
for (i in seq_along(twitter_names)) {
user_id_list[[i]] <- lookup_users(twitter_names[i])$user_id
user_followers[[i]] <- get_followers(twitter_names[i])
user_friends[[i]] <- get_friends(twitter_names[i])
}
i
user_id_list
rate_limits()
View(rate_limits())
user_id_list
user_followers
user_friends
user_followers
do.call(user_followers, rbind.fill)
do.call(rbind.fill, user_followers)
foo <- do.call(rbind.fill, user_followers)
View(foo)
head(user_followers)
foo <- do.call(rbind.fill, user_friends)
user_friends
foo <- do.call(rbind.fill, user_friends[1:14])
View(foo)
foo <- do.call(rbind.fill, user_friends[1:14])
View(fo)
View(foo)
table(foo$user_id) %>% sort
table(foo$user_id) %>% sort(decreasing = T)51:10
table(foo$user_id) %>% sort(decreasing = T)[1:10]
table(foo$user_id) %>% sort(decreasing = T)
table(foo$user_id) %>% sort(decreasing = T) %>% .$[1:10]
table(foo$user_id) %>% sort(decreasing = T) %>% .[1:10]
as_screenname(252087644)
lookup_users(252087644)
user_id_list
user_id_list
twitter_names <- c("AnaKubli", "annapellegatta", "caromatamoros", "cusimanof",
"dgohla", "jonvrushi", "luciacizmaziova", "nadinaiacob",
"PresRamirez", "RubenZoest", "simonsaysnothin", "sjash87",
"donata64", "RummelJa", "bernstmeng")
user_id_list <- list()
user_followers <- list()
user_friends <- list()
for (i in seq_along(twitter_names)) {
user_id_list[[i]] <- lookup_users(twitter_names[i])$user_id
user_followers[[i]] <- get_followers(twitter_names[i])
user_friends[[i]] <- get_friends(twitter_names[i])
}
user_id_df <- data.frame(twitter_names, unlist(user_id_list), stringsAsFactors = FALSE)
user_id_df
user_id_df <- data.frame(name = twitter_names, user_id = unlist(user_id_list), stringsAsFactors = FALSE)
user_id_df
user_friends_df <- do.call(rbind.fill, user_friends)
View(user_friends_df)
user_followers
user_followers[[1]]
unlist(user_followers)
user_followers[[i]]
sapply(user_followers[[i]], class)
user_followers_list <- list()
for(i in seq_along(twitter_names)) {
user_followers_list[[i]] <- data.frame(name = twitter_names[i], user_followers = user_followers[[i]], stringsAsFactors = FALSE)
}
user_followers_list
user_followers[[i]]
user_followers_list <- list()
for(i in seq_along(twitter_names)) {
user_followers_list[[i]] <- data.frame(name = twitter_names[i], user_followers = user_followers[[i]]$user_id, stringsAsFactors = FALSE)
}
user_followers_list
user_followers_df <- do.call(rbind.fill, user_friends)
user_followers_df
names(user_friends_df)
head(user_friends_df)
head(v)
head(user_id_df)
names(user_friends_df) <- c("name", "friend_id")
user_friends_df <- merge(user_friends_df, user_id_df, by = "name", all.x = TRUE)
# user friends df
user_friends_df <- do.call(rbind.fill, user_friends)
names(user_friends_df) <- c("name", "friend_id")
user_friends_df <- merge(user_friends_df, user_id_df, by = "name", all.x = TRUE)
head(user_friends_df)
# user followers df
user_followers_list <- list()
for(i in seq_along(twitter_names)) {
user_followers_list[[i]] <- data.frame(name = twitter_names[i], user_followers = user_followers[[i]]$user_id, stringsAsFactors = FALSE)
}
user_followers_df <- do.call(rbind.fill, user_friends)
names(user_followers_df)
names(user_followers_df) <- c("name", "follower_id")
user_followers_df <- merge(user_followers_df, user_id_df, by = "name", all.x = TRUE)
head(user_followers_df)
user_id_df$user_id
user_friends_df <- filter(user_friends_df, friend_id %in% user_id_df$user_id)
View(user_friends_df)
user_followers_df <- filter(user_followers_df, follower_id %in% user_id_df$user_id)
user_followers_df
user_friends_df
dim(user_followers_df)
# user id df
user_id_df <- data.frame(name = twitter_names, user_id = unlist(user_id_list), stringsAsFactors = FALSE)
# user friends df
user_friends_df <- do.call(rbind.fill, user_friends)
names(user_friends_df) <- c("name", "friend_id")
user_friends_df <- merge(user_friends_df, user_id_df, by = "name", all.x = TRUE)
# user followers df
user_followers_list <- list()
for(i in seq_along(twitter_names)) {
user_followers_list[[i]] <- data.frame(name = twitter_names[i], user_followers = user_followers[[i]]$user_id, stringsAsFactors = FALSE)
}
user_followers_df <- do.call(rbind.fill, user_friends)
names(user_followers_df) <- c("name", "follower_id")
user_followers_df <- merge(user_followers_df, user_id_df, by = "name", all.x = TRUE)
dim(user_followers_df)
dim(user_friends_df)
# user friends df
user_friends_df <- do.call(rbind.fill, user_friends)
names(user_friends_df) <- c("name", "friend_id")
user_friends_df <- merge(user_friends_df, user_id_df, by = "name", all.x = TRUE)
# user followers df
user_followers_list <- list()
for(i in seq_along(twitter_names)) {
user_followers_list[[i]] <- data.frame(name = twitter_names[i], user_followers = user_followers[[i]]$user_id, stringsAsFactors = FALSE)
}
user_followers_df <- do.call(rbind.fill, user_followers)
names(user_followers_df) <- c("name", "follower_id")
user_followers_df <- merge(user_followers_df, user_id_df, by = "name", all.x = TRUE)
user_followers
head(user_followers)
# user followers df
user_followers_list <- list()
for(i in seq_along(twitter_names)) {
user_followers_list[[i]] <- data.frame(name = twitter_names[i], user_followers = user_followers[[i]]$user_id, stringsAsFactors = FALSE)
}
user_followers_df <- do.call(rbind.fill, user_followers_list)
names(user_followers_df) <- c("name", "follower_id")
user_followers_df <- merge(user_followers_df, user_id_df, by = "name", all.x = TRUE)
dim(user_followers_df)
head(user_friends_df)
table(user_friends_df$friend_id)
table(user_friends_df$friend_id) %>% sort %>% .[1:10]
table(user_friends_df$friend_id) %>% sort(decreasing = T) %>% .[1:10]
table(user_friends_df$friend_id) %>% sort(decreasing = T) %>% .[1:10] %>% names
lookup_users("252087644")
lookup_users("252087644")$name
lookup_users("813286")$name
lookup_users("127908397")$name
lookup_users("5988062")$name
