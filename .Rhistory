setwd("/Users/simonmunzert/GitHub/rscraping-hertie-2018")
url <- "http://www.spiegel.de/schlagzeilen/"
url_out <- GET(url, add_headers(from = "eddie@datacollection.com"))
datetime <- str_replace_all(Sys.time(), "[ :]", "-")
content(url_out, as = "text") %>% write(file = str_c("data/spiegelHeadlines/headlines-spiegel-", datetime, ".html"))
content(url_out, as = "text") %>% write(file = str_c("data/spiegelHeadlines/headlines-spiegel-", datetime, ".html"))
url_out
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl start spiegelheadlines")
system("launchctl list")
library(rvest)
library(httr)
library(stringr)
url_parsed <- read_html("http://www.google.com")
html_form(url_parsed)
search <- html_form(url_parsed)[[1]]
search
form <- set_values(search, q = "kneipen kreuzberg")
form
google_search <- submit_form(url_parsed, form)
google_search <- submit_form("http://www.google.com", form)
google_search <- submit_form(search, form)
form
?submit_form
session <- html_session("http://www.google.com")
session
google_search <- submit_form(session, form)
?submit_form
google_search
url_parsed <- read_html(google_search)
hits_text <- html_nodes(url_parsed, xpath = "//*[@class='r']//a") %>% html_text()
hits_links <- html_nodes(url_parsed, xpath = "//*[@class='r']//a") %>% html_attr("href")
hits_text
hits_links
url <- "http://wordnetweb.princeton.edu/perl/webwn"
browseURL(url)
url_parsed <- read_html(url)
html_form(url_parsed)
uastring <- "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"
session <- html_session(url, user_agent(uastring))
wordnet_form <- set_values(wordnet, s = "data")
wordnet <- html_form(url_parsed)[[1]]
wordnet_form <- set_values(wordnet, s = "data")
wordnet_search <- submit_form(session, wordnet_form)
url_parsed <- read_html(wordnet_search)
url_parsed %>% html_nodes(xpath = "//li") %>% html_text()
url <- "http://read-able.com/"
browseURL(url)
url_parsed <- read_html(url)
html_form(url_parsed)
readable <- html_form(url_parsed)[[2]]
sentence <- '"It is a capital mistake to theorize before one has data. Insensibly one begins to twist facts to suit theories, instead of theories to suit facts." - Arthur Conan Doyle, Sherlock Holmes'
readable_form <- set_values(readable, directInput = sentence)
readable_form
url <- "http://read-able.com/"
session <- html_session(url, user_agent(uastring))
readable_search <- submit_form(session, readable_form)
url_parsed <- read_html(readable_search)
html_table(url_parsed)
browseURL("https://developer.apple.com/library/content/documentation/MacOSX/Conceptual/BPSystemStartup/Chapters/ScheduledJobs.html")
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl start spiegelheadlines")
system("launchctl list")
system("launchctl stop spiegelheadlines")
system("launchctl unload ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl load ~/Library/LaunchAgents/spiegelheadlines.plist")
system("launchctl start spiegelheadlines")
system("launchctl list")
browseURL("http://www.jstatsoft.org/")
session <- html_session("http://www.google.com")
library(rvest)
library(httr)
library(stringr)
session <- html_session("http://www.google.com")
session
search <- html_form(session)[[1]]
html_form(session)
search <- html_form(session)[[1]]
form <- set_values(search, q = "kneipen kreuzberg")
form
google_search <- submit_form(session, form)
?submit_form
url_parsed <- read_html(google_search)
hits_text <- html_nodes(url_parsed, xpath = "//*[@class='r']//a") %>% html_text()
hits_links <- html_nodes(url_parsed, xpath = "//*[@class='r']//a") %>% html_attr("href")
hits_text
hits_links
url <- "http://wordnetweb.princeton.edu/perl/webwn"
browseURL(url)
url <- "http://wordnetweb.princeton.edu/perl/webwn"
url_parsed <- read_html(url)
html_form(url_parsed)
wordnet <- html_form(url_parsed)[[1]]
uastring <- "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"
session <- html_session(url, user_agent(uastring))
wordnet_form <- set_values(wordnet, s = "data")
wordnet_search <- submit_form(session, wordnet_form)
url_parsed <- read_html(wordnet_search)
url_parsed %>% html_nodes(xpath = "//li") %>% html_text()
session <- html_session(url, user_agent(uastring))
wordnet_form <- set_values(wordnet, s = "data", o2 = "1")
wordnet_search <- submit_form(session, wordnet_form)
url_parsed <- read_html(wordnet_search)
url_parsed %>% html_nodes(xpath = "//li") %>% html_text()
url <- "http://read-able.com/"
url_parsed <- read_html(url)
html_form(url_parsed)
readable <- html_form(url_parsed)[[2]]
sentence <- '"It is a capital mistake to theorize before one has data. Insensibly one begins to twist facts to suit theories, instead of theories to suit facts." - Arthur Conan Doyle, Sherlock Holmes'
readable_form <- set_values(readable, directInput = sentence)
readable_form
session <- html_ses sion(url, user_agent(uastring))
session <- html_session(url, user_agent(uastring))
readable_search <- submit_form(session, readable_form)
url_parsed <- read_html(readable_search)
html_table(url_parsed)
source("packages.r")
## setup R + RSelenium -------------------------
# install current version of Java SE Runtime Environment
browseURL("http://www.oracle.com/technetwork/java/javase/downloads/jre8-downloads-2133155.html")
# load RSelenium
library(RSelenium)
# set up connection via RSelenium package
# documentation: http://cran.r-project.org/web/packages/RSelenium/RSelenium.pdf
# check currently installed version of Java
system("java -version")
#system("java -jar selenium-server-standalone-3.4.0.jar")
source("packages.r")
## Staying friendly on the web ------
# work with informative header fields
# don't bombard server
# respect robots.txt
# add header fields with httr::GET
browseURL("http://httpbin.org")
GET("http://httpbin.org/headers")
GET("http://httpbin.org/headers", add_headers(`User-Agent` = R.Version()$version.string))
GET("http://httpbin.org/headers", add_headers(From = "my@email.com"))
GET("http://httpbin.org/headers", add_headers(From = "my@email.com",
`User-Agent` = R.Version()$version.string))
# example
url_response <- GET("http://spiegel.de/schlagzeilen",
add_headers(From = "my@email.com"))
url_parsed <- url_response  %>% read_html()
url_parsed %>% html_nodes(".schlagzeilen-headline") %>%  html_text()
# add header fields with rvest + httr
url <- "http://spiegel.de/schlagzeilen"
session <- html_session(url, add_headers(From = "my@email.com"))
headlines <- session %>% html_nodes(".schlagzeilen-headline") %>%  html_text()
# respect robots.txt
browseURL("https://www.google.com/robots.txt")
browseURL("http://www.nytimes.com/robots.txt")
library(robotstxt)
# more info see here: https://cran.r-project.org/web/packages/robotstxt/vignettes/using_robotstxt.html
paths_allowed("/", "http://google.com/", bot = "*")
paths_allowed("/", "https://facebook.com/", bot = "*")
r_text <- get_robotstxt("https://www.google.com/")
r_parsed <- parse_robotstxt(r_text)
names(r_parsed)
View(r_parsed)
table(r_parsed$permissions$useragent, r_parsed$permissions$field)
URLdecode("file:///Users/simonmunzert/Munzert%20Dropbox/Simon%20Munzert/Uni/Teaching/SS%202018/Web%20Data%20Collection/assignments/05-scraping-II-solution.html")
source("packages.r")
# install current version of Java SE Runtime Environment
browseURL("http://www.oracle.com/technetwork/java/javase/downloads/jre8-downloads-2133155.html")
library(RSelenium)
system("java -version")
rD <- rsDriver()
remDr <- rD[["client"]]
url <- "http://www.iea.org/policiesandmeasures/renewableenergy/"
remDr$navigate(url)
xpath <- '//*[@id="main"]/div/form/div[1]/ul/li[1]/span'
regionsElem <- remDr$findElement(using = 'xpath', value = xpath)
regionsElem$clickElement() # click on button
regionsElem$clickElement() # click on button
xpath <- '//*[@id="main"]/div/form/div[1]/ul/li[1]/ul/li[5]/label/input'
regionsElem$clickElement() # click on button
regionsElem$clickElement() # click on button
regionsElem$clickElement() # click on button
xpath <- '//*[@id="main"]/div/form/div[1]/ul/li[1]/ul/li[5]/label/input'
euElem <- remDr$findElement(using = 'xpath', value = xpath)
selectEU <- euElem$clickElement() # click on button
xpath <- '//*[@id="main"]/div/form/div[5]/select[1]'
fromDrop <- remDr$findElement(using = 'xpath', value = xpath)
clickFrom <- fromDrop$clickElement() # click on drop-down menu
writeFrom <- fromDrop$sendKeysToElement(list("2000")) # enter start year
xpath <- '//*[@id="main"]/div/form/div[5]/select[2]'
toDrop <- remDr$findElement(using = 'xpath', value = xpath)
clickTo <- toDrop$clickElement() # click on drop-down menu
writeTo <- toDrop$sendKeysToElement(list("2010")) # enter end year
xpath <- '//*[@id="main"]/div/form/button[2]'
searchElem <- remDr$findElement(using = 'xpath', value = xpath)
resultsPage <- searchElem$clickElement() # click on button
output <- remDr$getPageSource(header = TRUE)
write(output[[1]], file = "iea-renewables.html")
content <- read_html("iea-renewables.html", encoding = "utf8")
tabs <- html_table(content, fill = TRUE)
tab <- tabs[[1]]
names(tab) <- c("title", "country", "year", "status", "type", "target")
head(tab)
remDr$closeServer()
R.Version()$version.string
url <- "http://spiegel.de/schlagzeilen"
session <- html_session(url, add_headers(From = "my@email.com", ))
session <- html_session(url, add_headers(From = "my@email.com"))
headlines <- session %>% html_nodes(".schlagzeilen-headline") %>%  html_text()
paths_allowed("/", "http://google.com/", bot = "*")
paths_allowed("/", "https://facebook.com/", bot = "*")
r_text <- get_robotstxt("https://www.google.com/")
r_parsed <- parse_robotstxt(r_text)
names(r_parsed)
table(r_parsed$permissions$useragent, r_parsed$permissions$field)
# use ready-made binding, the aRxiv package
library(aRxiv)
source("packages.r")
# API documentation
browseURL("http://ip-api.com/")
# ipapi package
devtools::install_github("hrbrmstr/ipapi")
library(ipapi)
# function call
ip_df <- geolocate(c(NA, "", "10.0.1.1", "", "72.33.67.89", "www.spiegel.de", "search.twitter.com"), .progress=TRUE)
geolocate()
ip_df
# function call
ip_df <- geolocate(c(NA, "", "10.0.1.1", "72.33.67.89", "www.spiegel.de", "search.twitter.com"), .progress=TRUE)
# function call
ip_df <- geolocate(c(NA, "", "10.0.1.1", "72.33.67.89", "www.spiegel.de", "search.twitter.com"), .progress=TRUE)
View(ip_df)
forecast <- read_xml("http://export.arxiv.org/api/query?search_query=all:forecast")
xml_ns(forecast) # inspect namespaces
authors <- xml_find_all(forecast, "//d1:author", ns = xml_ns(forecast))
authors %>% xml_text()
# use ready-made binding, the aRxiv package
library(aRxiv)
arxiv_df <- arxiv_search(query = "forecast AND submittedDate:[2016 TO 2017]", limit = 200, output_format = "data.frame")
View(arxiv_df)
arxiv_count('au:"Gary King"')
query_terms
arxiv_count('abs:"political" AND submittedDate:[2016 TO 2017]')
polsci_articles <- arxiv_search('abs:"political" AND submittedDate:[2016 TO 2017]', limit = 200)
browseURL("http://ip-api.com/")
devtools::install_github("hrbrmstr/ipapi")
library(ipapi)
?geolocate
ip_df <- geolocate(c(NA, "", "10.0.1.1", "72.33.67.89", "www.spiegel.de", "search.twitter.com"), .progress=TRUE)
View(ip_df)
names(ip_df)
browseURL("http://arxiv.org/help/api/index")
browseURL("http://arxiv.org/help/api/user-manual")
library(aRxiv)
arxiv_df <- arxiv_search(query = "forecast AND submittedDate:[2016 TO 2017]", limit = 200, output_format = "data.frame")
View(arxiv_df)
query_terms
browseURL("http://ropensci.org/")
browseURL("https://github.com/ropensci/opendata")
browseURL("https://cran.r-project.org/web/views/WebTechnologies.html")
browseURL("http://ip-api.com/docs/")
url <- "http://ip-api.com/xml/"
ip_parsed <- xml2::read_xml(url)
ip_parsed
ip_list <- as_list(ip_parsed)
source("packages.r")
ip_list <- as_list(ip_parsed)
ip_list
ip_list %>% unlist
class(ip_list %>% unlist)
ip_list %>% unlist %>% t
ip_list %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
url <- "http://ip-api.com/json"
ip_parsed <- jsonlite::fromJSON(url)
ip_parsed
ip_parsed <- jsonlite::fromJSON(url, flatten = TRUE)
ip_parsed
ip_parsed %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
ip_parsed %>% as.data.frame(ip_parsed, stringsAsFactors = FALSE)
fromJSON("http://ip-api.com/json/72.33.67.89") %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
fromJSON("http://ip-api.com/json/www.spiegel.de") %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
ipapi_grabber <- function(ip = "") {
dat < fromJSON(paste0("http://ip-api.com/json/", ip)) %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
dat
}
ipapi_grabber("72.33.67.89")
ipapi_grabber <- function(ip = "") {
dat <- fromJSON(paste0("http://ip-api.com/json/", ip)) %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
dat
}
ipapi_grabber("72.33.67.89")
?t
a
a <- matrix(1:30, 5, 6)
a
t(a)
167*100*52
browseURL("http://ip-api.com/")
library(ipapi)
?geolocate
c(NA, "", "10.0.1.1", "72.33.67.89", "www.spiegel.de", "search.twitter.com")
ip_df <- geolocate(c(NA, "", "10.0.1.1", "72.33.67.89", "www.spiegel.de", "search.twitter.com"), .progress=TRUE)
ip_df
View(ip_df)
browseURL("http://ropensci.org/")
browseURL("https://github.com/ropensci/opendata")
library(ggmap)
geocode("Berlin")
geocode("Berlin")
url <- "http://ip-api.com/xml/"
ip_parsed <- xml2::read_xml(url)
ip_parsed
ip_list <- as_list(ip_parsed)
source("packages.r")
ip_list <- as_list(ip_parsed)
ip_list
ip_list %>% unlist
?t
a <- matrix(1:30, 5, 6)
a
t(a)
foo <- ip_list %>% unlist
class(foo)
foo
ip_list %>% unlist %>% t
foo <- ip_list %>% unlist %>% t
class(foo)
foo <- ip_list %>% unlist
dim(foo)
ip_list %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
fromJSON("http://ip-api.com/json/72.33.67.89")
fromJSON("http://ip-api.com/json/72.33.67.89") %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
ipapi_grabber <- function(ip = "") {
dat <- fromJSON(paste0("http://ip-api.com/json/", ip)) %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
dat
}
ipapi_grabber("72.33.67.89")
?scale
library("xml2")
library("dplyr")
library("rvest")
library("stringr")
library(stringi)
stri_escape_unicode("\u00b5")
stri_unescape_unicode("\u00b5")
stri_unescape_unicode("<\u00b5>")
library(stringr)
library(stringi)
library(rvest)
stri_unescape_unicode("\u00b5")
symbol <- stri_unescape_unicode("\u00b5")
stri_escape_unicode(symbol)
?stri_unescape_unicode
song <- c(
"How many roads must a man walk down
Before you call him a man?
Yes, ’n’ how many seas must a white dove sail
Before she sleeps in the sand?
Yes, ’n’ how many times must the cannonballs fly
Before they’re forever banned?
The answer, my friend, is blowin’ in the wind
The answer is blowin’ in the wind
How many years can a mountain exist
Before it’s washed to the sea?
Yes, ’n’ how many years can some people exist
Before they’re allowed to be free?
Yes, ’n’ how many times can a man turn his head
Pretending he just doesn’t see?
The answer, my friend, is blowin’ in the wind
The answer is blowin’ in the wind
How many times must a man look up
Before he can see the sky?
Yes, ’n’ how many ears must one man have
Before he can hear people cry?
Yes, ’n’ how many deaths will it take till he knows
That too many people have died?
The answer, my friend, is blowin’ in the wind
The answer is blowin’ in the wind")
song
str_replace_all(song, pattern = "[:alpha:]{3,4}", replacement = "")
str_replace(song, pattern = "[:alpha:]{3,4}", replacement = "")
str_replace_all(song, pattern = "man|friend", replacement = "dog")
str_replace_all(song, pattern = "\\bman\\b|\\bfriend\\b", replacement = "dog")
str_replace_all(song, pattern = "\\bman\\b|\\bfriend\\b", replacement = "dog") %>% cat()
str_replace_all(song, pattern = "\\bman\\b|\\bfriend\\b", replacement = "dog") %>% print()
str_replace_all(song, pattern = "\\bman\\b|\\bfriend\\b", replacement = "dog") %>% cat()
?cat
str_replace_all(song, pattern = "\\bman\\b|\\bfriend\\b", replacement = "dog") %>% cat()
str_c(unlist(str_extract_all(gedichtl, "[[:upper:]]")), collapse="")
str_c(unlist(str_extract_all(song, "[[:upper:]]")), collapse="")
str_replace_all(song, "\\<[[:alpha:]]{1,3}\\>", "bla")
str_replace_all(song, "\\b[:alpha:]{1,3}\\b", "bla")
str_replace_all(song, "\\b[:alpha:]{5,}\\b", "bla")
str_split(song, "\\n")
str_split(song, "\\n") %>% str_trim()
str_split(song, "\\n") %>% sapply(str_trim())
str_split(song, "\\n")
?str_split
str_split(song, "\\n", simplify = TRUE) %>% str_trim()
verses <- str_split(song, "\\n", simplify = TRUE) %>% str_trim()
str_extract(verses, "^[:alpha:]+")
str_extract_all(verses, "^[:alpha:]+|[:alpha:]+.$")
url <- "https://en.wikipedia.org/wiki/List_of_World_Heritage_in_Danger"
url_parsed <- read_html(url)
tables <- html_table(url_parsed, fill = TRUE)
danger_table <- tables[[2]]
head(danger_table)
danger_table <- tables[[2]]
danger_table <- danger_table[,c(1,3,4,6,7)]
colnames(danger_table) <- c("name","locn","crit","yins","yend")
danger_table$name[1:3]
danger_table$yins
class(danger_table$yins)
danger_table$yend
danger_table$yend
yend_clean <- unlist(str_extract_all(danger_table$yend, "^[[:digit:]]{4}"))
yend_clean
danger_table$yend <- as.numeric(yend_clean)
danger_table$locn[c(1,3,5)]
danger_table$locn[c(1,3,5)]
danger_table$locn[1:5]
country <- str_extract(danger_table$locn, "[[:alpha:] ]+(?=[[:digit:]])") # use forward assertion in Perl regular expression
country
danger_table$locn
country
danger_table$locn
reg_y <- "[/][ -]*[[:digit:]]*[.]*[[:digit:]]*[;]"
reg_x <- "[;][ -]*[[:digit:]]*[.]*[[:digit:]]*"
y_coords <- str_extract(danger_table$locn, reg_y)
y_coords
y_coords <- str_extract(danger_table$locn, reg_y) %>% str_sub(3, -2)
y_coords
y_coords <- str_extract(danger_table$locn, reg_y) %>% str_sub(3, -2) %>% as.numeric()
danger_table$x_coords <- x_coords
x_coords <- str_extract(danger_table$locn, reg_x) %>% str_sub(3, -1) %>% as.numeric()
danger_table$x_coords <- x_coords
danger_table$locn <- NULL
head(danger_table)
danger_table <- tables[[2]]
danger_table
names(danger_table)
danger_table <- danger_table[,c(1,3,6,7)]
colnames(danger_table) <- c("name","locn","yins","yend")
head(danger_table)
par(oma=c(0,0,0,0))
par(mar=c(0,0,0,0))
points(danger_table$x_coords, danger_table$y_coords, pch = 19, col = "black", cex = .8)
map("world", col = "darkgrey", lwd = .5, mar = c(0.1,0.1,0.1,0.1))
library(maps)
map("world", col = "darkgrey", lwd = .5, mar = c(0.1,0.1,0.1,0.1))
points(danger_table$x_coords, danger_table$y_coords, pch = 19, col = "black", cex = .8)
box()
danger_table$x_coords
# select and rename columns
danger_table <- danger_table[,c(1,3,6,7)]
colnames(danger_table) <- c("name","locn","yins","yend")
# cleanse years
danger_table$yend
yend_clean <- unlist(str_extract_all(danger_table$yend, "^[[:digit:]]{4}"))
danger_table$yend <- as.numeric(yend_clean)
# get countries
danger_table$locn[1:5]
country <- str_extract(danger_table$locn, "[[:alpha:] ]+(?=[[:digit:]])")
country
danger_table$country <- country
# get coordinates
danger_table$locn[1:5]
reg_y <- "[/][ -]*[[:digit:]]*[.]*[[:digit:]]*[;]"
reg_x <- "[;][ -]*[[:digit:]]*[.]*[[:digit:]]*"
y_coords <- str_extract(danger_table$locn, reg_y) %>% str_sub(3, -2) %>% as.numeric()
danger_table$y_coords <- y_coords
x_coords <- str_extract(danger_table$locn, reg_x) %>% str_sub(3, -1) %>% as.numeric()
danger_table$x_coords <- x_coords
danger_table$locn <- NULL
map("world", col = "darkgrey", lwd = .5, mar = c(0.1,0.1,0.1,0.1))
points(danger_table$x_coords, danger_table$y_coords, pch = 19, col = "black", cex = 8)
map("world", col = "darkgrey", lwd = .5, mar = c(0.1,0.1,0.1,0.1))
points(danger_table$x_coords, danger_table$y_coords, pch = 19, col = "black", cex = .8)
box()
scale01 <- function(x) {
rng <- range(x, na.rm = TRUE)
(x - rng[1]) / (rng[2] - rng[1])
}
scale01
vec <- runif(10, 0, 10)
scale01(vec)
df <- mtcars
df[] <- lapply(df, scale01) # or:
df
lapply(df, scale01)
lapply(df, scale01) %>% as.data.frame
df <- iris
df_num <- sapply(df, is.numeric)
df
url <- "https://en.wikipedia.org/wiki/Fourth_Merkel_cabinet"
html_table(url)
read_html(url) %>% html_nodes("//table/tr/td[4]")
read_html(url) %>% html_nodes(xpath = "//table/tr/td[4]")
read_html(url) %>% html_nodes(xpath = "//table/tr/td[4]") %>% html_attr("href")
read_html(url) %>% html_nodes(xpath = "//table/tr/td[4]/a") %>% html_attr("href")
rel_links <- read_html("https://en.wikipedia.org/wiki/Fourth_Merkel_cabinet") %>% html_nodes(xpath = "//table/tr/td[4]/a") %>% html_attr("href")
rel_links
urls <- paste0("https://en.wikipedia.org", rel_links)
urls
dir.create("data/merkelcabinet")
?download.file
urls <- paste0("https://en.wikipedia.org", rel_links)
names <- paste0(basename(urls), ".html")
names
folder <- "data/merkelcabinet"
dir.create(folder)
folder <- "data/merkelcabinet/"
dir.create(folder)
pathnames <- paste0(folder, names)
pathnames
Map(download.file, urls, pathnames)
