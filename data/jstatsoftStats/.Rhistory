install.packages(c("BH", "broom", "devtools", "dplyr", "evaluate", "fields", "formatR", "geepack", "geosphere", "git2r", "highr", "httr", "jsonlite", "knitr", "lazyeval", "lme4", "MCMCpack", "mime", "miniCRAN", "nlme", "plyr", "psych", "quantreg", "RcppArmadillo", "rgdal", "rgeos", "rmarkdown", "rstudioapi", "rvest", "slam", "sp", "spatstat", "spdep", "survival", "tidyr", "useful", "VGAM", "WikidataR", "WikipediR", "withr", "XLConnect", "XLConnectJars", "xml2", "Zelig", "zoo"))
source("00-course-setup.r")
source("00-course-setup.r")
setwd("/Users/simon/Dropbox/Uni/ADCR/slides/slides-course-english")
library(rvest)
?html_nodes
?html_children
library(stringr)
example.obj <- "1. A small sentence. - 2. Another tiny sentence."
str_extract_all(example.obj, fixed("."))
?fixed
# load packages
library(rvest)
library(stringr)
library(ggmap)
library(maps)
cities_string <- read_html("https://en.wikipedia.org/wiki/Berlin") %>%
# extract the list items
html_nodes(xpath = "//*[@id='mw-content-text']//li[1]") %>%
html_text()
cities_string
cities_string <- read_html("https://en.wikipedia.org/wiki/Berlin") %>%
# extract the list items
html_nodes(xpath = "//div[@class='column-count-3']//li") %>%
html_text()
cities_string
cities_string <- read_html("https://en.wikipedia.org/wiki/Berlin") %>%
# extract the list items
html_nodes(xpath = "//div[contains(./@class)='column-count-3']//li") %>%
html_text()
cities_string <- read_html("https://en.wikipedia.org/wiki/Berlin") %>%
# extract the list items
html_nodes(xpath = "//div[contains()='column-count-3']//li") %>%
html_text()
cities_string <- read_html("https://en.wikipedia.org/wiki/Berlin") %>%
# extract the list items
html_nodes(xpath = "//div@class[contains()='column-count-3']//li") %>%
html_text()
cities_string <- read_html("https://en.wikipedia.org/wiki/Berlin") %>%
# extract the list items
html_nodes(xpath = "//div[contains()='column-count-3']//li") %>%
html_text()
cities_string <- read_html("https://en.wikipedia.org/wiki/Berlin") %>%
# extract the list items
html_nodes(xpath = "//div//li") %>%
html_text()
cities_string <- read_html("https://en.wikipedia.org/wiki/Berlin") %>%
# extract the list items
html_nodes(xpath = "//div[contains(@class, 'column-count-3')]//li") %>%
html_text()
cities_string
library(rtweet)
?search_tweets
library(stringr)
?str_subset
setwd("/Users/simonmunzert/GitHub/rscraping-hu-2017")
source("packages.r")
source("functions.r")
setwd(wd)
tempwd <- ("data/jstatsoftStats")
dir.create(tempwd)
setwd(tempwd)
browseURL("http://www.jstatsoft.org/")
# construct list of urls
baseurl <- "http://www.jstatsoft.org/article/view/v"
volurl <- paste0("0", seq(1,78,1))
volurl[1:9] <- paste0("00", seq(1, 9, 1))
brurl <- paste0("0", seq(1,9,1))
urls_list <- paste0(baseurl, volurl)
urls_list <- paste0(rep(urls_list, each = 9), "i", brurl)
names <- paste0(rep(volurl, each = 9), "_", brurl, ".html")
folder <- "html_articles/"
list_files <- list.files(folder, pattern = "0.*")
list_files_path <-  list.files(folder, pattern = "0.*", full.names = TRUE)
length(list_files)
files_size <- sapply(list_files_path, file.size)
table(files_size) %>% sort()
delete_files <- list_files_path[files_size == 23460]
sapply(delete_files, file.remove)
list_files_path <-  list.files(folder, pattern = "0.*", full.names = TRUE) # update list of files
# import pages and extract content
authors <- character()
title <- character()
statistics <- character()
numViews <- numeric()
datePublish <- character()
for (i in 1:length(list_files_path)) {
html_out <- read_html(list_files_path[i])
table_out <- html_table(html_out, fill = TRUE)[[6]]
authors[i] <- table_out[1,2]
title[i] <- table_out[2,2]
statistics[i] <- table_out[4,2]
numViews[i] <- statistics[i] %>% str_extract("[[:digit:]]+") %>% as.numeric()
datePublish[i] <- statistics[i] %>% str_extract("[[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}.$") %>% str_replace("\\.", "")
}
authors[1:3]
title[1:3]
numViews[1:3]
dat <- data.frame(authors = authors, title = title, numViews = numViews, datePublish = datePublish)
head(dat)
dim(dat)
plot(density(dat$numViews, from = 0), yaxt="n", ylab="", xlab="Number of views", main="Distribution of article page views in JSTATSOFT")
plot(density(dat$numViews, from = 0), yaxt="n", ylab="", xlab="Number of views", main="Distribution of article page views in JStatSoft")
ip_parsed <- jsonlite::fromJSON(url, flatten = TRUE)
url <- "http://ip-api.com/json"
ip_parsed <- jsonlite::fromJSON(url, flatten = TRUE)
ip_parsed
ip_parsed %>% unlist %>% t %>% as.data.frame(stringsAsFactors = FALSE)
ip_parsed
as.data.frame(ip_parsed)
ip_parsed <- jsonlite::fromJSON(url, flatten = TRUE)
ip_parsed
as.data.frame(ip_parsed)
as.data.frame(ip_parsed, stringsAsFactors = FALSE)
library(httr)
?oauth_endpoint
?sign_oauth1.0
?config
